---
title: "Executive summary and replication of 'Guiding or Following the Crowd?
Strategic communication as regulatory and reputational strategy' by Caelesta Braun and Moritz Müller"
author: "Moritz Muller"
date: "February 5, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Required packages
```{r}
library(tm.plugin.factiva)
library(tm)
library(dplyr)
library(quanteda)
library(tidytext)
library(tidyverse)
library(lubridate)
library(ldatuning)
library(topicmodels)
library(parallel)
library(vars)
library(portes)
library(tseries)
library(zoo)
library(boot)
```

This is the reproduction file for the paper "Guiding or Following the Crowd?
Strategic communication as regulatory and reputational strategy" by Caelesta Braun and Moritz Müller. Due to legal data ownership issues, we are not able to share the datasets that we have worked with. Instead, we show all the coding decisions and provide data summaries at an aggregate level. Furthermore, we provide the full code for the paper, including all the decisions that we have taken to arrive at the theoretical conclusions. We have run the full analysis with all the data and attach the output as html. Code is written by Moritz Müller.

Firstly, we load the required data and manipulate it according to our requirements for further analyses.
# 1. Load and format data
## 1.1. Scraped ECB data

```{r}
# ### Import and create dataset
# Interviews<-read.csv("data/ECB_Interviews_CSV.csv", encoding="UTF-8", strip.white = TRUE, header=TRUE)
# MDecisions <- read.csv("data/ECB_MonetaryDecision_CSV.csv", encoding="UTF-8", strip.white = TRUE, header=TRUE)
# ODecisions <- read.csv("data/ECB_OtherDecisions_CSV.csv", encoding="UTF-8", strip.white = TRUE, header=TRUE)
# PressConferences <- read.csv("data/ECB_Pressconferences_CSV.csv", encoding="UTF-8", strip.white = TRUE, header=TRUE)
# Speeches <- read.csv("data/ECB_Speeches_CSV_1.csv", encoding="UTF-8", strip.white = TRUE, header=TRUE)
# Speeches2 <- read.csv("data/ECB_Speeches_CSV_2.csv", encoding="UTF-8", strip.white = TRUE, header=TRUE)
# PressReleases <- read.csv("data/ECB_Pressreleases_CSV.csv", encoding="UTF-8", strip.white = TRUE, header=TRUE)
# 
# # Add Column of Type
# 
# I <- data.frame(Interviews, doc_id=paste("Interview_",seq.int(nrow(Interviews)),sep=""), type="Interview")
# MD <- data.frame(MDecisions, doc_id=paste("MDecisions_",seq.int(nrow(MDecisions)),sep=""), type="Monetary_Decisions")
# OD <- data.frame(ODecisions, doc_id=paste("ODecisions_",seq.int(nrow(ODecisions)),sep=""), type="Other_Decisions")
# PC <- data.frame(PressConferences, doc_id=paste("PressConferences_",seq.int(nrow(PressConferences)),sep=""), type="Press_Conferences", Language="English")
# names(PC)[2] <- "Title"
# S1 <- data.frame(Speeches, doc_id=paste("Speeches_",seq.int(nrow(Speeches)),sep=""), type="Speeches")
# names(S1)[1] <- "X.U.FEFF.Time"
# names(S1)[2] <- "Description1"
# S2 <- data.frame(Speeches2, doc_id=paste("Speeches2_",seq.int(nrow(Speeches2)),sep=""), type="Speeches")
# names(S2)[1] <- "X.U.FEFF.Time"
# names(S2)[2] <- "Description1"
# PR <- data.frame(PressReleases, doc_id=paste("PressReleases_",seq.int(nrow(PressReleases)),sep=""), type="Press_Releases")
# PR <- PR[!(PR$Title == "Monetary policy decisions"),]
# PR <- PR[!(PR$Title == "Monetary Policy Decisions"),]
# 
# # Merge Datasets into one
# Joined <- merge(x=I, y=PC, by = c("X.U.FEFF.Time", "Title", "Text", "Language", "doc_id", "type"), all.x=TRUE, all.y = TRUE)
# ### Joined <- merge(x=Joined, y=OD, by = c("X.U.FEFF.Time", "Title", "Text", "Language", "doc_id", "type"), all.x=TRUE, all.y = TRUE)
# ### Joined <- merge(x=Joined, y=PC, by = c("X.U.FEFF.Time", "Title", "Text", "Language", "doc_id", "type"), all.x=TRUE, all.y = TRUE)
# Joined <- merge(x=Joined, y=S1, by = c("X.U.FEFF.Time", "Text", "Language", "Description1", "Description2", "doc_id", "type"), all.x=TRUE, all.y = TRUE)
# Joined <- merge(x=Joined, y=S2, by = c("X.U.FEFF.Time", "Text", "Language", "Description1", "Description2", "doc_id", "type"), all.x=TRUE, all.y = TRUE)
# Joined <- merge(x=Joined, y=PR, by = c("X.U.FEFF.Time", "Title", "Text", "Language", "doc_id", "type"), all.x=TRUE, all.y = TRUE)
# 
# # Clean Dataset
# 
# Joined$Description <- paste(Joined$Description1, Joined$Description2)
# Joined$Description1 <- NULL
# Joined$Description2 <- NULL
# Joined$Description[Joined$Description == "NA NA"] <- "NA"
# 
# names(Joined)[3] <- "text"
# names(Joined)[2] <- "heading"
# names(Joined)[1] <- "datetimestamp"
# names(Joined)[4] <- "language"
# names(Joined)[7] <- "description"
# 
# #Joined$datetimestamp <- as.POSIXct(Joined$datetimestamp, tz= "", format = "%d %B %Y")
# Joined = Joined[,c(5,3,1,2,4,6,7)]
# 
# # only keep documents in English
# Joined = Joined[Joined$language == "English", ]
# Joined = Joined[!Joined$text == "", ]
# Joined = Joined[!Joined$datetimestamp == "", ]
# 
# #make sure that text column of the object is character class, otherwise quanteda won't be able to read it
# Joined$text = as.character(Joined$text)
# 
# ##save data
# #write.csv(Joined, "data/Joined.csv")
# #If using Joined object, make sure that text column is character string. Loading in the csv with stringsAsFactors = F gets around that as well.
# 
# ECB_quanteda = corpus(Joined)
# tidy = tidy(ECB_quanteda, row_names = T)
#save(ECB_quanteda, file = "ECB_quanteda")
# tidy$id = rownames(tidy)
# 
# #add dates to tidy for later analyses
# tidy$date = as.POSIXct(tidy$datetimestamp, tz= "", format = "%d %B %Y")
# tidy$quarter = lubridate::quarter(tidy$date, with_year=T)
# tidy$month = format(as.Date(tidy$date), "%Y-%m")
# tidy$year = format(as.Date(tidy$date), "%Y")
# 
# #add leadership variable (1=Duisenberg, 2=Trichet, 3=Draghi)
# tidy$leadership = "1"
# tidy$leadership[tidy$month > "2003-11"] = "2"
# tidy$leadership[tidy$month > "2011-11"] = "3"

#save(tidy, file = "tidy_ecb")

load(file = "tidy_ecb")
# show summary of data
summary(tidy)
```

# 1.2. Load Newspaper Article
```{r}
# # Read in Factiva html files via tm.plugin.factiva. A list solution is not possible, since tm does not allow the merging of multiple list objects into one Corpus.
# for (i in c(1:63)){
#   path = paste0("data/Factiva", i, ".html")
#   object = Corpus(FactivaSource(path, encoding="UTF8"))
#   names(object) = paste0("source", i)
# }
# # Combine all in one file 
# Corpus_Media<-c(source1,source2,source3,source4,source5,source6,source7,source8,source9,source10,source11,source12,source13,source14,source15,source16,source17,source18,source19,source20,source21,source22,source23,source24,source25,source26,source27,source28,source29,source30,source31,source32,source33,source34,source35,source36,source37,source38,source39,source40,source41,source42,source43,source44,source45,source46,source47,source48,source49,source50,source51,source52,source53,source54,source55,source56,source57,source58,source59,source60,source61,source62,source63)
# 
# # Cut articles with less than 150 words
# Corpus_Media = tm_filter(Corpus_Media, FUN = function(x) meta(x)[["wordcount"]] > 150)
# 
# # Create tidy Corpus to look into it
# tidy_m <- tidy(Corpus_Media)
# 
# # add dates and leadership variables
# tidy_m$date <- as.POSIXct(tidy_m$datetimestamp, tz= "", format = "%d %B %Y")
# tidy_m$quarter <- lubridate::quarter(tidy_m$date, with_year=T)
# tidy_m$month <- format(as.Date(tidy_m$date), "%Y-%m")
# tidy_m$year <- format(as.Date(tidy_m$date), "%Y")
# 
# #add leadership variable (1=Duisenberg, 2=Trichet, 3=Draghi)
# tidy_m$leadership <- "1"
# tidy_m$leadership[tidy_m$month > "2003-11"] <- "2"
# tidy_m$leadership[tidy_m$month > "2011-11"] <- "3"

# #add unique id
# tidy_m$UID <- c(1:5546)

#save(tidy_m, file = "tidy_newspapers")
# save same file without text for reproduction
write.csv(tidy_m[,c(1:7)], file="newspaper_identifiers.csv")
load(file = "tidy_newspapers")

# show summary of data
colnames(tidy_m)
```

# 2. Frequency of communication
## 2.1. ECB
```{r}
com_freq_type <- tidy %>%
  filter(year>2000&year<2018) %>% 
  group_by(type, year) %>%
  count (id, year) %>%
  summarize (count = sum(n))

ggplot(com_freq_type, aes(year, count)) + geom_col(show.legend = T, aes(fill = type)) +scale_fill_grey(name = "Type",labels = c( "Interviews", "Press Conferences", "Speeches", "Press Releases"))
```

## 2.2. Newspapers
```{r}
# Frequency of communication
tidy_m$origin[tidy_m$origin == "Financial Times (FT.Com)"] <- "Financial Times"
tidy_m$origin[tidy_m$origin == "Guardian Unlimited" | tidy_m$origin == "Guardian.co.uk"] <- "The Guardian"
tidy_m$origin[tidy_m$origin == "Israel Business Arena" | tidy_m$origin == "The Asian Wall Street Journal"
              | tidy_m$origin == "El País" | tidy_m$origin == "The Wall Street Journal (Asia Edition)"
              | tidy_m$origin == "The Wall Street Journal (Europe Edition)" | tidy_m$origin == "The Wall Street Journal (Online and Print)"
              | tidy_m$origin == "The Wall Street Journal Asia" | tidy_m$origin == "The Wall Street Journal Europe"
              | tidy_m$origin == "The Wall Street Journal Online" | tidy_m$origin == "WSJ Pro Bankruptcy"
              | tidy_m$origin == "WSJ Pro Central Banking" | tidy_m$origin == "WSJ Pro Financial Regulation"] <- "The Wall Street Journal"

com_freq_type <- tidy_m %>%
  filter(year>2000) %>% 
  group_by(origin,year) %>%
  count (id, year) %>%
  summarize (count = sum(n))
# Plot frequency
ggplot(com_freq_type, aes(year, count)) + geom_col(show.legend = T, aes(fill = origin))+scale_fill_grey(name = "Origin")
```

# 3. Topic Models
## 3.1.1. clean ECB corpus
```{r}
# Clean punctuation, remove stopwords, remove numbers, and stem
load(file = "ECB_quanteda")
dfm_k <- dfm(ECB_quanteda, remove_punct = T, remove = c(stopwords('english'), stopwords('spanish'), stopwords('german'), "press", "release", "website", "http",
                                                    "public statement", "twitterfacebooklinkedingooglepluswhatsappemail", 
                                                    "/t", "\t", "european", "bank", "ecb", "also", "euro", "also", "can", "central"), remove_numbers = T, stem = T, verbose = T)
dfm_k <- dfm_trim(dfm_k, min_docfreq = 2, verbose = T)
#delete empty rows
rowTotals <- apply(dfm_k, 1, sum)
dfm_s <- dfm_k[rowTotals>0,]
dfm_s
```
## 3.1.2. Pass ECB corpus to topicmodel simulation. Check optimal number of topics
```{r}
# Find optimal number of topics with ldatuning package. NOTE: We ran a simulation with 0-100 topics in steps of 5 before and limited it to this range. For completeness sake, we also add the earlier simulation (running it might take up to 40 minutes, depending on number of cores)
# detectCores()
# set.seed(1234)
# #result_first <- FindTopicsNumber(
# #   dfm_s,
# #   topics = seq(from = 5, to = 100, by = 5),
# #   metrics = c("Griffiths2004", "CaoJuan2009", "Deveaud2014"),
# #   method = "Gibbs",
# #   control = list(verbose=T, seed = 1234, burnin = 50, iter = 200),
# #   mc.cores = 7,
# #   verbose = TRUE
# # )
# result <- FindTopicsNumber(
#   dfm_s,
#   topics = seq(from = 32, to = 40, by = 2),
#   metrics = c("Griffiths2004", "CaoJuan2009", "Deveaud2014"),
#   method = "Gibbs",
#   control = list(verbose=T, seed = 1234, burnin = 50, iter = 200),
#   mc.cores = detectCores()-1,
#   verbose = TRUE
# )

#save(result, file = "result")
load (file= "result")

# Plot results
#FindTopicsNumber_plot(result_first)
FindTopicsNumber_plot(result)

```
## 3.1.3. Run topicmodel simulation
```{r}
#create topic model (38 topics is a good amount, we tried with other options and found this to be the most intelligable option)
# set.seed(1993)
# K <- 38
# TheMother <- LDA(dfm_s, k = K, method = "Gibbs", 
#                         control = list(verbose=25L, seed = 1234, burnin = 100, iter = 500))

#save(TheMother, file = "TheMother")
load((file = "TheMother"))
terms_ecb = as.data.frame(terms(TheMother, 10))

terms_ecb
#save(TheMother, file = "data/ECBTopicModel")
```
## 3.1.4. Explore and identify topics
```{r}
#find right topic. We are using an example here related to core competencies. Based on the literature
probs <- exp(TheMother@beta[,TheMother@terms=="stabil"])
# normalizing so that probabilities add up to 1
round(probs/sum(probs),3)

#create a df with topic words and colnames with topic per word probabilities
##stability
probs <- round(exp(TheMother@beta[,TheMother@terms=="stabil"])/sum(exp(TheMother@beta[,TheMother@terms=="stabil"])),3)
terms_stability <- as.data.frame(get_terms(TheMother, 50))
colnames(terms_stability) <- round(probs/sum(probs),3)

##new responsibilities
probs <- round(exp(TheMother@beta[,TheMother@terms=="supervisori"])/sum(exp(TheMother@beta[,TheMother@terms=="supervisori"])),3)
terms_newresp <- as.data.frame(get_terms(TheMother, 10))
colnames(terms_newresp) <- round(probs/sum(probs),3)

##unconventional measures
probs <- round(exp(TheMother@beta[,TheMother@terms=="unconvent"])/sum(exp(TheMother@beta[,TheMother@terms=="unconvent"])),3)
terms_uncon <- as.data.frame(get_terms(TheMother, 50))
colnames(terms_uncon) <- round(probs/sum(probs),3)
```
## 3.1.4. Check topic presence across time
```{r}
# Calculate average per year per document topic occurrence to get a feel for how the topic behaves. To illustrate, topic 23 is the strongest associated to core responsibilities (see df terms_stability)
agg <- list()
for (i in 1:38){
  tidy$prob_topic <- TheMother@gamma[,i]
  agg[[length(agg)+1]] <- aggregate(prob_topic~year, data=tidy, FUN=mean)
}

# ggplot
ggplot(agg[[23]], aes(year, prob_topic, group = 1)) + geom_point() + geom_line() + ylab("average per-document occurrence") + ggtitle("LDA-based measurement of price stability in ECB communication")
```
## 3.1.5. Check for topicmodel stability, NOTE: NOT WORKING ANYMORE (PACKAGE UPDATE)
```{r}
# #check for topic stability, so generate 30 more 
# validation_models <- list()
# for (i in 1:30){
#   SNR <- as.integer(i)
#   validation_models[[length(validation_models)+1]] <- LDA(dfm_n, k = K, method = "Gibbs", 
#                                                           control = list(verbose=25L, seed = SNR, burnin = 100, iter = 500))
# }
# 
# testitest <- validation_models[[1]]
# beta_1 <- as.matrix(testitest@beta)
# beta_2 <- as.matrix(validation_models[[2]]@beta)
# 
# library(textmineR)
# CalcJSDivergence(beta_1, by_rows= FALSE)
```

## 3.2. clean News Corpus
```{r}
# Further clean corpus before running topic models: Remove whitespaces, lowercase, remove stopwords (also corpus specific), stem, remove Punctuation. For the task at hand (creating topic models), the removed features do not add any valuable information and only complicate calculating the topic models
#save(Corpus_Media, file = "Corpus_Media")
load(file = "Corpus_Media")
news_corpus_clean <- tm_map(Corpus_Media, content_transformer(tolower))
news_corpus_clean <- tm_map(news_corpus_clean, removeWords,
                            stopwords("english"))
news_corpus_clean <- tm_map(news_corpus_clean, stripWhitespace)
news_corpus_clean <- tm_map(news_corpus_clean, stemDocument)
news_corpus_clean <- tm_map(news_corpus_clean, removePunctuation)

T_Corpus <- tidy(news_corpus_clean)%>%
  unnest_tokens(word, text)

Frequencies <- T_Corpus%>%
  count(word, sort=TRUE) %>%
  ungroup()

# Edited Corpus with additional deleted terms and date and tidy
news_corpus_clean_2<-tm_map(news_corpus_clean, removeWords, c("bank", "said", "mr.", "european", "central", "ecb"))
T_Corpus_2 <- tidy(news_corpus_clean_2)%>% 
  unnest_tokens(word, text)
T_Corpus_2_complete<-tidy(news_corpus_clean_2)
T_Corpus_2_complete$date <- as.POSIXct(T_Corpus_2_complete$datetimestamp, tz= "", format = "%Y-%m-%d")
T_Corpus_2_complete$quarter <- lubridate::quarter(T_Corpus_2_complete$date, with_year=T)
T_Corpus_2_complete$month <- format(as.Date(T_Corpus_2_complete$date), "%Y-%m")
T_Corpus_2_complete$year <- format(as.Date(T_Corpus_2_complete$date), "%Y")

```
## 3.2.1. Robustness: Run topic model on news corpus
```{r}
# news_corpus <- corpus(T_Corpus_2_complete)
# news_dfm <- dfm(news_corpus, verbose = T)
# news_dfm <- dfm_trim(news_dfm, min_docfreq = 3)
# news_dfm
# 
# # TOPIC MODEL to creat dictionary for frames
# # LDA
# K <- 38
# robustness_newstopics <- LDA(dfm, k = K, method = "Gibbs", 
#            control = list(verbose=25L, seed = 123, burnin = 100, iter = 500))
#save(robustness_newstopics, file = "robustness_newstopics")
load(file = "robustness_newstopics")
terms_newscorpus = as.data.frame(terms(robustness_newstopics, 10))
#lda_check <- LDA(dfm, model = lda, control = list(seed=1234, estimate.beta=FALSE))

terms_newscorpus
```

# 4. Dictionary construction
```{r eval=F}
### I am using the same object names, so first run ECB and then run then Newspaper data 
#################ECB frame detection#############################################################################

library(dplyr)
library(tidyverse)
library(tidytext)

### Frame definition
frame_conventional <- c("stability", "stabil", "stabl", "stable", "stabilityori", "stabilis", "stabilised", "stabilisers", "inflat", "rate", "price", "currenc", "currency", "currencies")
frame_unconventional <- c("measur", "stimulus", "purchas", "quantitativ", "eas", "unconvent", "unconventional", "conven", "convent", "convention", "conventional", "ass", "bond", "buy")
frame_expansion <- c("supervis", "supervi", "SSM", "new", "expan", "mandat", "mandate", "mandates", "respons", "responsibility", "responsibility", "macroprudenti", "macro-prudent", "macro-prudenti")

dict <- dictionary(list(frame_conventional = c("stability", "stabil", "stabl", "stable", "stabilityori", "stabilis", "stabilised", "stabilisers", "inflat", "rate", "price", "currenc", "currency", "currencies"),
                        frame_unconventional = c("measur", "stimulus", "purchas", "quantitativ", "eas", "unconvent", "unconventional", "conven", "convent", "convention", "conventional", "ass", "bond", "buy"),
                        frame_expansion = c("supervis", "supervi", "SSM", "new", "expan", "mandat", "mandate", "mandates", "respons", "responsibility", "responsibility", "macroprudenti", "macro-prudent", "macro-prudenti")))
# as percentage of total 

# EXPERIMENTAL: Quanteda approach is MUCH easier and faster, but uses a different stemmer so the dictionary does not work properly
dict_ECB <- dfm_lookup(dfm_s, dictionary = dict)
dict_ECB <- convert(dict_ECB, to = "data.frame")
tidy$count_total <- rowSums(dfm_s)
tidy[,c("Con","Uncon","Exp")] <- dict_ECB[,2:4]
tidy$ratioConventional<- tidy$Con/tidy$count_total
tidy$ratioUnconventional<- tidy$Uncon/tidy$count_total
tidy$ratioExpansion<- tidy$Exp/tidy$count_total

agg_ECB <-
  aggregate(
    list(
      conventional = tidy$ratioConventional,
      unconcentional = tidy$ratioUnconventional,
      expansion = tidy$ratioExpansion
    ),
    by = list(year = tidy$year),
    data = tidy,
    FUN = mean
  )

agg_ECB_month = aggregate(
  list(
    conventional = tidy$ratioConventional,
    unconventional = tidy$ratioUnconventional,
    expansion = tidy$ratioExpansion
  ),
  by = list(year = tidy$month),
  data = tidy,
  FUN = mean
)


load("data/Clean_Tidy_Corpus")
Frequency <- T_ECB_complete %>%
  unnest_tokens(word, text)
word_frequencies <- Frequency %>%
  count(word, id) %>%
  ungroup()%>%
  bind_tf_idf(word, id, n)
total_words <- word_frequencies%>%
  group_by(id)%>%
  summarize(total = sum(n))
word_frequencies <- left_join(word_frequencies, total_words)
word_frequencies <- word_frequencies %>%
  mutate(percentage=(n/total))

##take word counts
total_c <- word_frequencies %>%
  filter(word %in% frame_conventional)%>%
  ungroup() %>%
  mutate(type="c")

total_u <- word_frequencies %>%
  filter(word %in% frame_unconventional)%>%
  ungroup() %>%
  mutate(type="u")

total_e <- word_frequencies %>%
  filter(word %in% frame_expansion)%>%
  ungroup() %>%
  mutate(type="e")

##attach dates
dates <- T_ECB_complete[c("id", "date", "quarter", "month", "year")]

total1_c <- inner_join(total_c, dates, by="id")
total1_u <- inner_join(total_u, dates, by="id")
total1_e <- inner_join(total_e, dates, by="id")

##conventional
total2_c <- total1_c%>%
  group_by(id, year, month)%>%
  summarize(overall =sum(tf))%>%
  ungroup()
total2_c_y <- total2_c %>%
  group_by(year) %>%
  summarize(average = mean(overall))%>%
  ungroup()
total2_c_m <- total2_c %>%
  group_by(month) %>%
  summarize(average = mean(overall))%>%
  ungroup()

##unconventional
total2_u <- total1_u%>%
  group_by(id, year, month)%>%
  summarize(overall =sum(tf))%>%
  ungroup()
total2_u_y <- total2_u %>%
  group_by(year) %>%
  summarize(average = mean(overall))%>%
  ungroup()
total2_u_m <- total2_u %>%
  group_by(month) %>%
  summarize(average = mean(overall))%>%
  ungroup()

##expansion
total2_e <- total1_e%>%
  group_by(id, year, month)%>%
  summarize(overall =sum(tf))%>%
  ungroup()
total2_e_y <- total2_e %>%
  group_by(year) %>%
  summarize(average = mean(overall))%>%
  ungroup()
total2_e_m <- total2_e %>%
  group_by(month) %>%
  summarize(average = mean(overall))%>%
  ungroup()

total_all_y <- data.frame(year = total2_c_y$year, c = total2_c_y$average, u = total2_u_y$average, e = total2_e_y$average)
total_all_m <- full_join(total2_c_m, total2_u_m, by = "month")
total_all_m <- full_join(total_all_m, total2_e_m, by = "month")
names(total_all_m)[2] <- "c"
names(total_all_m)[3] <- "u"
names(total_all_m)[4] <- "e"


##plot yearly averages
ggplot(total2_c_y, aes(year, average, group =1)) +
  geom_point(show.legend = FALSE) + geom_line()+ 
  ylab("average per-document occurence")

ggplot(total2_u_y, aes(year, average)) +
  geom_col(show.legend = FALSE)

ggplot(total2_e_y, aes(year, average)) +
  geom_col(show.legend = FALSE)

ggplot(total_all_y, aes(year)) + 
  geom_line(aes(y=c, colour = "Core responsibilities"), group = 1) + geom_point(aes(y=c, colour = "Core responsibilities"), group = 1)+
  geom_line(aes(y = u, colour = "Unconventional measures"), group=2) + geom_point(aes(y=u, colour = "Unconventional measures"), group = 2)+
  geom_line(aes(y = e, colour = "Expansion of responsibilities"), group=3) + geom_point(aes(y=e, colour = "Expansion of responsibilities"), group = 3)+
  ggtitle("Ratio of frame words in ECB communication") +
  ylab("average per-document occurence")

##plot monthly averages
ggplot(total2_c_m, aes(month, average)) +
  geom_point(show.legend = FALSE)

ggplot(total2_u_m, aes(month, average)) +
  geom_col(show.legend = FALSE)

ggplot(total2_e_m, aes(month, average)) +
  geom_col(show.legend = FALSE)

ggplot(total_all_m, aes(month)) + 
  geom_line(aes(y=c, colour = "Core responsibilities"), group = 1) + 
  geom_line(aes(y = u, colour = "Unconventional measures"), group=2) +
  geom_line(aes(y = e, colour = "Expansion of responsibilities"), group=3) +
  ggtitle("Percentage of frames in ECB communication") 

##start only at 1999
total_all_m1999 <- total_all_m[23:256,]

ggplot(total_all_m1999, aes(month)) + 
  geom_line(aes(y=c, colour = "Core responsibilities"), group = 1) + 
  geom_line(aes(y = u, colour = "Unconventional measures"), group=2) +
  geom_line(aes(y = e, colour = "Expansion of responsibilities"), group=3) +
  ggtitle("Percentage of frames in ECB communication") 


### Save series
#save(total_all_m1999, file = "data/ECB_mframes_1999_tidy1")
#save(total_all_m, file = "data/ECB_mframes_raw1")
#save(total_all_y, file = "data/ECB_yframes_raw1")

#################Newspaper frame detetion#############################################################################
#Word frequencies
load("data/tidy_corpus")
T_Corpus_2_complete$UID <- c(1:5546)
T_Corpus_2 <- T_Corpus_2_complete %>%
  unnest_tokens(word, text)
Frequency <- T_Corpus_2%>%
  count(word)
word_frequencies <- T_Corpus_2%>%
  count(UID, word, sort=TRUE) %>%
  ungroup()%>%
  bind_tf_idf(word, UID, n)
total_words <- word_frequencies%>%
  group_by(UID)%>%
  summarize(total = sum(n))
word_frequencies <- left_join(word_frequencies, total_words)
word_frequencies <- word_frequencies %>%
  mutate(percentage=(n/total))

## assemble frames, based on the lda words
frame_conventional <- c("stability", "stabil", "stabl", "stable", "stabilityori", "stabilis", "stabilised", "stabilisers", "inflat", "rate", "price", "currenc", "currency", "currencies")
frame_unconventional <- c("measur", "stimulus", "purchas", "quantitativ", "eas", "unconvent", "unconventional", "conven", "convent", "convention", "conventional", "ass", "bond", "buy")
frame_expansion <- c("supervis", "supervi", "SSM", "new", "expan", "mandat", "mandate", "mandates", "respons", "responsibility", "responsibility", "macroprudenti")


## word frequencies
total_c <- word_frequencies %>%
  filter(word %in% frame_conventional)%>%
  ungroup() %>%
  mutate(type="c")

total_u <- word_frequencies %>%
  filter(word %in% frame_unconventional)%>%
  ungroup() %>%
  mutate(type="u")

total_e <- word_frequencies %>%
  filter(word %in% frame_expansion)%>%
  ungroup() %>%
  mutate(type="e")

dates <- T_Corpus_2_complete[c("id", "date", "quarter", "month", "year", "datetimestamp", "UID", "text")]

total1_c <- inner_join(total_c, dates, by="UID")
total1_u <- inner_join(total_u, dates, by="UID")
total1_e <- inner_join(total_e, dates, by="UID")

##conventional
total2_c <- total1_c%>%
  group_by(UID, year, month)%>%
  summarize(overall =sum(tf))%>%
  ungroup()
total2_c_y <- total2_c %>%
  group_by(year) %>%
  summarize(average = mean(overall))%>%
  ungroup()
total2_c_m <- total2_c %>%
  group_by(month) %>%
  summarize(average = mean(overall))%>%
  ungroup()

##unconventional
total2_u <- total1_u%>%
  group_by(UID, year, month)%>%
  summarize(overall =sum(tf))%>%
  ungroup()
total2_u_y <- total2_u %>%
  group_by(year) %>%
  summarize(average = mean(overall))%>%
  ungroup()
total2_u_m <- total2_u %>%
  group_by(month) %>%
  summarize(average = mean(overall))%>%
  ungroup()

##expansion
total2_e <- total1_e%>%
  group_by(UID, year, month)%>%
  summarize(overall =sum(tf))%>%
  ungroup()
total2_e_y <- total2_e %>%
  group_by(year) %>%
  summarize(average = mean(overall))%>%
  ungroup()
total2_e_m <- total2_e %>%
  group_by(month) %>%
  summarize(average = mean(overall))%>%
  ungroup()

News_total_all_y <- data.frame(year = total2_c_y$year, c = total2_c_y$average, u = total2_u_y$average, e = total2_e_y$average)
News_total_all_m <- full_join(total2_c_m, total2_u_m, by = "month")
News_total_all_m <- full_join(total_all_m, total2_e_m, by = "month")
names(News_total_all_m)[2] <- "c"
names(News_total_all_m)[3] <- "u"
names(News_total_all_m)[4] <- "e"
News_total_all_m$average <- NULL

###Plot yearly averages
ggplot(total2_c_y, aes(year, average)) +
  geom_col(show.legend = FALSE)

ggplot(total2_u_y, aes(year, average)) +
  geom_col(show.legend = FALSE)

ggplot(total2_e_y, aes(year, average)) +
  geom_col(show.legend = FALSE)

ggplot(News_total_all_y, aes(year)) + 
  geom_line(aes(y=c, colour = "Core responsibilities"), group = 1) + 
  geom_line(aes(y = u, colour = "Unconventional measures"), group=2) +
  geom_line(aes(y = e, colour = "Expansion of responsibilities"), group=3) +
  ggtitle("Ratio of frame words in Media communication")

ggplot(News_total_all_y, aes(year)) + 
  geom_line(aes(y=c, colour = "Core responsibilities"), group = 1) + geom_point(aes(y=c, colour = "Core responsibilities"), group = 1)+
  geom_line(aes(y = u, colour = "Unconventional measures"), group=2) + geom_point(aes(y=u, colour = "Unconventional measures"), group = 2)+
  geom_line(aes(y = e, colour = "Expansion of responsibilities"), group=3) + geom_point(aes(y=e, colour = "Expansion of responsibilities"), group = 3)+
  ggtitle("Ratio of frame words in Media communication") +
  ylab("average per-document occurence")
##plot monthly averages
ggplot(total2_c_m, aes(month, average)) +
  geom_point(show.legend = FALSE)

ggplot(total2_u_m, aes(month, average)) +
  geom_col(show.legend = FALSE)

ggplot(total2_e_m, aes(month, average)) +
  geom_col(show.legend = FALSE)


ggplot(total_all_m, aes(month)) + 
  geom_line(aes(y=c, colour = "Core responsibilities"), group = 1) + 
  geom_line(aes(y = u, colour = "Unconventional measures"), group=2) +
  geom_line(aes(y = e, colour = "Expansion of responsibilities"), group=3) +
  ggtitle("Percentage of frames in Media communication") 

### Save series
#save(News_total_all_y, file = "data/News_yframes_raw1")
#save(News_total_all_m, file = "data/News_mframes_raw1")
```
# 5. VAR analysis 
## 5.1. Transfer counts to time series data
```{r eval=F}
library(zoo)
library(dplyr)

# Load datasets
## ECB
load("data/ECB_frames1")
ECB.frames <- abc ## Is already in time series format

load("data/ECB_mframes_1999_tidy1") ## still in tidy
## Newspaper corpus
load("data/News_mframes_raw1")
News_total_all_m

names(News_total_all_m)[2] <- "News.c"
names(News_total_all_m)[3] <- "News.u"
names(News_total_all_m)[4] <- "News.e"

## add Financial stress indicator (CLIFS, origin:ECB)
load("data/FSI_month")
names(FS)[1] <- "month"
FS$Year <- NULL

#add ECB leadership
leadership_values <- aggregate(as.numeric(leadership)~month, data=tidy, FUN=mean)
total_all_m1999 <- full_join(total_all_m1999, leadership_values,by="month")
## Combine datasets
complete <- full_join(total_all_m1999, News_total_all_m, by = "month")
complete <- complete[25:228,]
complete <- full_join(complete, FS, by = "month")
names(complete)[5] <- "leadership"
complete$leadership <- as.factor(complete$leadership)
## Replace NAs with priors
complete <- na.locf(complete, fromLast = FALSE)

##add economic policy uncertainty score (policyuncertainty.com)
PU <- read.csv(file = "data/Policy_Uncertainty_Data.csv", header=T, sep= ",", stringsAsFactors = F)
PU$Year <- as.numeric(PU$Year)
PU <- PU[(PU$Year >= 2001) & (PU$Year < 2018) & (!is.na(PU$Year)),]
complete$PU <- PU$European_News_Index
complete$binaryPU <- ifelse(complete$PU>150, 1, 0)

##add systemic stress indicator (CISS, origin:ECB)
CISS <- read.csv(file = "data/CISS.csv", header=T, sep= ",", stringsAsFactors = F)
colnames(CISS) <- c("date", "CISS")
CISS$date <- as.Date(CISS$date, format = "%m/%d/%Y")
CISS$month <- format(CISS$date, "%Y-%m")
CISS.average <- CISS %>%
  group_by(month)%>%
  mutate(CISS = mean(CISS))%>%
  distinct(month, CISS)
CISS.average <- arrange(CISS.average, -row_number())
complete$CISS <- CISS.average$CISS
## save dataframe
#save(complete, file = "data/all_measures_dataframe1")
#load("data/all_measures_dataframe1")
```
## 5.2. Conduct VAR analysis
```{r eval=F}
# VAR
## Creating time series datasets
complete$month <- as.yearmon(CISS.average$month, "%Y-%m")
complete.df <- as.data.frame(complete)
rownames(complete.df) <- complete.df$month
complete.df$month <- NULL

test <- ts(complete.df)
time_series <- ts(complete.df, start = c(2001,1), frequency = 12)
plot(time_series[,c(1:6)], main = "Frame developments")
legend("bottomright", c("test", "3"))
differences <- diff(time_series, start = c(2001,1), frequency = 12)
plot(differences)
#save(time_series, file = "data/time_series1")
load("data/time_series1")

# plot smoothed time series
library(TTR)
names <- c("ECB:Core", "ECB:Unconventional", "ECB:Expansion", "News:Core", "News:Unconventional", "News:Expansion")
smooth <- as.data.frame(SMA(time_series[,1], n=6))
for (i in 1:6){
  smooth[,i] <- as.data.frame(SMA(time_series[,i], n=6))
}
colnames(smooth) <- names
smooth <- ts(smooth, start = c(2001,1), frequency = 12)

# Figure 2
#save(smooth, file = "smooth_timeseries")
load(file = "smooth_timeseries")
plot(smooth, main=" ", xlab= " ", cex.lab =0.8)
title(main ="Frame development", sub="y-axis displays fraction of frame words amongst overall wordscount")

## as quaterly data
ts_agg <-  aggregate(time_series, nfrequency=4, mean)
ts_agg <- ts(ts_agg, frequency = 1)
##Compare crisis indicators
plot(time_series[,c(7,8,9)], main = "Financial climate measures")
## run VAR with monthly data
###check stationarity (Dickey-Fuller)
library(TTR)
library(tseries)
tseries::adf.test(time_series[,7], alternative = "stationary", k=0)
library(aTSA)
stationary.test(time_series[,7], nlag = 3, method = "adf")

## check autocorrelation with portmanneau 
library(portes)
portest(var.1)
###VAR
library(vars)
var.1 <- VAR(time_series,type="none", lag.max=2) # Estimate the model
var.aic <- VAR(time_series[,c(1,4)],type="none",lag.max=3,ic="AIC", exogen = time_series[,c(2,3,5,6)])
summary(var.aic)

causality(var.aic, cause = "News.c")

###Check accuracy by comparing predicted and real
round(rbind(coef(var.aic)[[1]][,1],coef(var.aic)[[2]][,1]),2)
plot(var.aic)


###check results
summary(var.aic, equation = "PU")
summary(var.aic, equation = "u")

## causality test
###subset c, e, u
subset.c <- ts(data.frame(ECB.core_responsibilities = complete$c, News.core_responsibilities = complete$News.c))
subset.e <- ts(data.frame(ECB.expansion_responsibilities = complete$e, News.expansion_responsibilities = complete$News.e))
subset.u <- ts(data.frame(ECB.unconventional_measures = complete$u, News.unconventional_measures = complete$News.u))
subset.cFSI <- ts(data.frame(c = complete$c, FSI = complete$FSI))
subset.eFSI <- ts(data.frame(e = complete$e, FSI = complete$FSI))
subset.uFSI <- ts(data.frame(u = complete$u, FSI = complete$FSI))
subset.cPU <- ts(data.frame(c = complete$c, PU = complete$PU))
subset.ePU <- ts(data.frame(e = complete$e, PU = complete$PU))
subset.uPU <- ts(data.frame(u = complete$u, PU = complete$PU))
subset.NewscFSI <- ts(data.frame(News.c = complete$News.c, FSI = complete$FSI))
subset.NewseFSI <- ts(data.frame(News.e = complete$News.e, FSI = complete$FSI))
subset.NewsuFSI <- ts(data.frame(News.u = complete$News.u, FSI = complete$FSI))
subset.NewscPU <- ts(data.frame(News.c = complete$News.c, PU = complete$PU))
subset.NewsePU <- ts(data.frame(News.e = complete$News.e, PU = complete$PU))
subset.NewsuPU <- ts(data.frame(News.u = complete$News.u, PU = complete$PU))

v.c <- VAR(subset.c, type="none", lag.max = 3, ic="AIC", exogen = cbind(test = complete$PU))
v.e <- VAR(subset.e, type="none", lag.max = 3, ic="AIC", exogen = cbind(test = complete$PU))
v.u <- VAR(subset.u, type="none", lag.max = 3, ic="AIC", exogen = cbind(test = complete$PU))
v.c <- VAR(subset.c, type="none", lag.max = 3, ic="AIC")
v.e <- VAR(subset.e, type="none", lag.max = 3, ic="AIC")
v.u <- VAR(subset.u, type="none", lag.max = 3, ic="AIC")

v.cFSI <- VAR(subset.cFSI, type="none", lag.max = 3, ic="AIC")
v.eFSI <- VAR(subset.eFSI, type="none", lag.max = 3, ic="AIC")
v.uFSI <- VAR(subset.uFSI, type="none", lag.max = 3, ic="AIC")
v.NewscFSI <- VAR(subset.NewscFSI, type="none", lag.max = 3, ic="AIC")
v.NewseFSI <- VAR(subset.NewseFSI, type="none", lag.max = 3, ic="AIC")
v.NewsuFSI <- VAR(subset.NewsuFSI, type="none", lag.max = 3, ic="AIC")
v.cPU <- VAR(subset.cPU, type="none", lag.max = 3, ic="AIC")
v.ePU <- VAR(subset.ePU, type="none", lag.max = 3, ic="AIC")
v.uPU <- VAR(subset.uPU, type="none", lag.max = 3, ic="AIC")
v.NewscPU <- VAR(subset.NewscPU, type="none", lag.max = 3, ic="AIC")
v.NewsePU <- VAR(subset.NewsePU, type="none", lag.max = 3, ic="AIC")
v.NewsuPU <- VAR(subset.NewsuPU, type="none", lag.max = 3, ic="AIC")

summary(v.c)
summary(v.e)
summary(v.u)
summary(v.cPU)
summary(v.ePU)
summary(v.uPU)
summary(v.NewscPU)
summary(v.NewsePU)
summary(v.NewsuPU)

###causality tests
causality(v.c, cause = "ECB.core_responsibilities")
causality(v.c, cause = "News.core_responsibilities")
causality(v.e, cause = "ECB.expansion_responsibilities")
causality(v.e, cause = "News.expansion_responsibilities")
causality(v.u, cause = "ECB.unconventional_measures")
causality(v.u, cause = "News.unconventional_measures")

causality(v.cPU, cause = "c")
causality(v.cPU, cause = "PU")
causality(v.ePU, cause = "e")
causality(v.ePU, cause = "PU")
causality(v.uPU, cause = "u")
causality(v.uPU, cause = "PU")

causality(v.NewscPU, cause = "News.c")
causality(v.NewscPU, cause = "PU")
causality(v.NewsePU, cause = "News.e")
causality(v.NewsePU, cause = "PU")
causality(v.NewsuPU, cause = "News.u")
causality(v.NewsuPU, cause = "PU")

#impulse response function
#Create irf, then set shocksize(one forecast error variance) of impulse variable to 100 to get percentage change of response variable
set.seed(1234)

#ECB.core_responsilities
irf_vc = irf(v.c, impulse = "ECB.core_responsibilities", response = "News.core_responsibilities", ortho=T)
irf_vc$irf$ECB.core_responsibilities <- (irf_vc$irf$ECB.core_responsibilities/summary(v.c)$varresult$News.core_responsibilities$sigma)*100
irf_vc$Lower$ECB.core_responsibilities <-(irf_vc$Lower$ECB.core_responsibilities/summary(v.c)$varresult$News.core_responsibilities$sigma)*100
irf_vc$Upper$ECB.core_responsibilities <-(irf_vc$Upper$ECB.core_responsibilities/summary(v.c)$varresult$News.core_responsibilities$sigma)*100
plot(irf_vc)

#News.core_responsilities
irf_vc = irf(v.c, impulse = "News.core_responsibilities", response = "ECB.core_responsibilities", ortho=T)
irf_vc$irf$News.core_responsibilities <- (irf_vc$irf$News.core_responsibilities/summary(v.c)$varresult$ECB.core_responsibilities$sigma)*100
irf_vc$Lower$News.core_responsibilities <-(irf_vc$Lower$News.core_responsibilities/summary(v.c)$varresult$ECB.core_responsibilities$sigma)*100
irf_vc$Upper$News.core_responsibilities <-(irf_vc$Upper$News.core_responsibilities/summary(v.c)$varresult$ECB.core_responsibilities$sigma)*100
plot(irf_vc)

#ECB.expansion_responsilities
irf_ve = irf(v.e, impulse = "ECB.expansion_responsibilities", response = "News.expansion_responsibilities", ortho=T)
irf_ve$irf$ECB.expansion_responsibilities <- (irf_ve$irf$ECB.expansion_responsibilities/summary(v.e)$varresult$News.expansion_responsibilities$sigma)*100
irf_ve$Lower$ECB.expansion_responsibilities <-(irf_ve$Lower$ECB.expansion_responsibilities/summary(v.e)$varresult$News.expansion_responsibilities$sigma)*100
irf_ve$Upper$ECB.expansion_responsibilities <-(irf_ve$Upper$ECB.expansion_responsibilities/summary(v.e)$varresult$News.expansion_responsibilities$sigma)*100
plot(irf_ve)

#News.expansion_responsibilities
irf_ve = irf(v.e, impulse = "News.expansion_responsibilities", response = "News.expansion_responsibilities", ortho=T)
irf_ve$irf$News.expansion_responsibilities <- (irf_ve$irf$News.expansion_responsibilities/summary(v.e)$varresult$ECB.expansion_responsibilities$sigma)*100
irf_ve$Lower$News.expansion_responsibilities <-(irf_ve$Lower$News.expansion_responsibilities/summary(v.e)$varresult$ECB.expansion_responsibilities$sigma)*100
irf_ve$Upper$News.expansion_responsibilities <-(irf_ve$Upper$News.expansion_responsibilities/summary(v.e)$varresult$ECB.expansion_responsibilities$sigma)*100
plot(irf_ve)

#ECB.unconventional_measures
irf_vu = irf(v.u, impulse = "ECB.unconventional_measures", response = "News.unconventional_measures", ortho=T)
irf_vu$irf$ECB.unconventional_measures <- (irf_vu$irf$ECB.unconventional_measures/summary(v.u)$varresult$News.unconventional_measures$sigma)*100
irf_vu$Lower$ECB.unconventional_measures <-(irf_vu$Lower$ECB.unconventional_measures/summary(v.u)$varresult$News.unconventional_measures$sigma)*100
irf_vu$Upper$ECB.unconventional_measures <-(irf_vu$Upper$ECB.unconventional_measures/summary(v.u)$varresult$News.unconventional_measures$sigma)*100
plot(irf_vu)

#News.unconventional_responsilities
irf_vu = irf(v.u, impulse = "News.unconventional_measures", response = "News.unconventional_measures", ortho=T)
irf_vu$irf$News.unconventional_measures <- (irf_vu$irf$News.unconventional_measures/summary(v.u)$varresult$ECB.unconventional_measures$sigma)*100
irf_vu$Lower$News.unconventional_measures <-(irf_vu$Lower$News.unconventional_measures/summary(v.u)$varresult$ECB.unconventional_measures$sigma)*100
irf_vu$Upper$News.unconventional_measures <-(irf_vu$Upper$News.unconventional_measures/summary(v.u)$varresult$ECB.unconventional_measures$sigma)*100
plot(irf_vu)



summary(v.c)$varresult$News.core_responsibilities$sigma
plot(irf(v.c, impulse = "News.core_responsibilities", response = "ECB.core_responsibilities", ortho = T))

plot(irf(v.e, impulse = "ECB.expansion_responsibilities", response = "News.expansion_responsibilities"))
plot(irf(v.e, impulse = "News.expansion_responsibilities", response = "ECB.expansion_responsibilities"))

plot(irf(v.u, impulse = "ECB.unconventional_measures", response = "News.unconventional_measures"))
plot(irf(v.u, impulse = "News.unconventional_measures", response = "ECB.unconventional_measures"))

normality.test(v.c)
data(Canada)
var.2c <- VAR(Canada, p = 2, type = "const")
normality.test(var.2c)
plot(v.c)
```
###rolling Granger causality test
####Create rolling timeseries set and do granger test on subsets
```{r eval=F}
rollingseries <- list()
rollingseries_dif <- list()
grangerz <- data.frame(c=as.numeric(1:192),nc=as.numeric(1:192),u=as.numeric(1:192),nu=as.numeric(1:192), e=as.numeric(1:192), ne=as.numeric(1:192), stringsAsFactors = F)
grangerz_dif <- data.frame(c=as.numeric(1:192),u=as.numeric(1:192),e=as.numeric(1:192), stringsAsFactors = F)
for(i in 1:nrow(time_series)){
  x <- i
  y <- i+12
  rollingseries[[i]] <- time_series[x:y,]
  if (y >= nrow(time_series)) { break }
}
for(i in 1:nrow(differences)){
  x <- i
  y <- i+12
  rollingseries_dif[[i]] <- differences[x:y,]
  if (y >= nrow(differences)) { break }
}
for (i in 1:length(rollingseries)){
  grangerz[i,] <- c(c=as.numeric(causality(VAR(ts(as.data.frame(rollingseries[[i]])[,c(1,4)]), type="none", lag.max=3, ic="AIC"), cause="c")[[1]][3]),
                    nc=as.numeric(causality(VAR(ts(as.data.frame(rollingseries[[i]])[,c(1,4)]), type="none", lag.max=3, ic="AIC"), cause="News.c")[[1]][3]),
                    u=as.numeric(causality(VAR(ts(as.data.frame(rollingseries[[i]])[,c(2,5)]), type="none", lag.max=3, ic="AIC"), cause="u")[[1]][3]),
                    nu=as.numeric(causality(VAR(ts(as.data.frame(rollingseries[[i]])[,c(2,5)]), type="none", lag.max=3, ic="AIC"), cause="News.u")[[1]][3]),
                    e=as.numeric(causality(VAR(ts(as.data.frame(rollingseries[[i]])[,c(3,6)]), type="none", lag.max=3, ic="AIC"), cause="e")[[1]][3]),
                    ne=as.numeric(causality(VAR(ts(as.data.frame(rollingseries[[i]])[,c(3,6)]), type="none", lag.max=3, ic="AIC"), cause="News.e")[[1]][3]))
}
for (i in 1:length(rollingseries_dif)){
  grangerz_dif[i,] <- c(c=as.numeric(causality(VAR(ts(as.data.frame(rollingseries_dif[[i]])[,c(1,4)]), type="none", lag.max=3, ic="AIC"), cause="News.c")[[1]][3]),
                        u=as.numeric(causality(VAR(ts(as.data.frame(rollingseries_dif[[i]])[,c(2,5)]), type="none", lag.max=3, ic="AIC"), cause="News.u")[[1]][3]),
                        e=as.numeric(causality(VAR(ts(as.data.frame(rollingseries_dif[[i]])[,c(3,6)]), type="none", lag.max=3, ic="AIC"), cause="News.e")[[1]][3]))
}

##Create complete dataset with all attached pvalues for the Granger tests
empty <- data.frame(c=rep("0.24",6),  nc = rep("0.0047",6), u=rep("0.24",6), nu = rep("0.60",6), e=rep("0.52",6),  ne = rep("0.10",6), stringsAsFactors = F)
empty2 <- data.frame(c=rep("0.59",6), nc = rep("0.0018",6), u=rep("0.29",6), nu = rep("0.016",6), e=rep("0.32",6), ne = rep("0.072",6), stringsAsFactors = F)
final <- rbind(empty, grangerz)
final <- rbind(final, empty2)
names(final) <- c("p.c","p.nc","p.u","p.nu","p.e", "p.ne")
complete <- data.frame(complete.df, final)
complete$date <- seq.Date(from=base::as.Date("2001-01-01"), to=base::as.Date("2017-12-01"), by="month")
#complete_ts <- ts(complete_rs, start=c(2001,1), frequency=12)
#plot(complete_ts[,1:10])
#complete_rs$date <- seq.Date(from=base::as.Date("2001-01-01"), to=base::as.Date("2017-12-01"), by="month")
#complete[,1:10] <- as.numeric(complete[,1:10])
#complete$sig <- ifelse(complete$p.c > 0.05, 0, 1)
#save(complete, file="data/complete_new")
#load("data/complete_new")


#Plot significant timeframes
complete$year <- substr(complete$date,1,4)
complete$sig_c = as.numeric(ifelse(complete$p.c > 0.05, 0, 1))
complete$sig_u = as.numeric(ifelse(complete$p.u > 0.05, 0, 1))
complete$sig_e = as.numeric(ifelse(complete$p.e > 0.05, 0, 1))
complete$sig_nc = as.numeric(ifelse(complete$p.nc > 0.05, 0, 1))
complete$sig_nu = as.numeric(ifelse(complete$p.nu > 0.05, 0, 1))
complete$sig_ne = as.numeric(ifelse(complete$p.ne > 0.05, 0, 1))
sig_count <- complete %>%
  group_by(year) %>%
  summarise(ECB_core = sum(sig_c), ECB_uncon = sum(sig_u), ECB_exp = sum(sig_e), News_core = sum(sig_nc), News_uncon = sum(sig_nu), News_exp = sum(sig_ne))

write.csv(sig_count, file="results/rollinggranger.csv")
#Test if a regression shows significance between granger causality and 
summary(glm(sig_ne~FSI, data=complete, family=binomial))

#plot signigicance
fit <- lm(p.c ~ FSI, data = complete)
summary(fit)
plot(fit)
#Plot significant timeframes
complete$year <- substr(complete$date,1,4)
sig_count <- complete %>%
  group_by(year) %>%
  summarise(total = sum(sig))

d <- data.frame(differences=diff(complete$PU, start = c(2001,2), frequency = 12))
d$date <- seq.Date(from=base::as.Date("2001-02-01"), to=base::as.Date("2017-12-01"), by="month")
d$year <- substr(d$date,1,4)

crisismonths <- d %>%
  mutate(cri = ifelse(differences>20, 1, 0)) %>%
  group_by(year) %>%
  summarize(total = sum(cri)) 

sig_count$crisis <- crisismonths$total

plot(ts(sig_count[,c(2,3)], start = 2001, frequency = 1))

#test if crisis is triggering frame congruence
testset <- data.frame(date = complete$date, sig=complete$sig, p.c= 1-(complete$p.c))
testset$crisis <- is.na(1:204)
testset$crisis[2:204] <- crisismonths$cri
var.test <- VAR(ts(testset[,c(3,4)]), lag.max=2, type="none", ic="AIC")
summary(var.test)
causality(var.test, cause = "p.c")


##plot the whole thing with p-values as background color
library(ggfortify)
library(ggplot2)
ggplot(data=complete, aes(date,c)) +
  geom_rect(aes(NULL, NULL, xmin=as.Date('2001-01-01')-1,xmax=as.Date('2017-12-01')+1, fill = ifelse(complete$sig=="TRUE", "green", "red")),ymin=0,ymax=0.6)+
  geom_line() + geom_point(shape=ifelse(complete$sig=="TRUE", 1, 2), data = complete)
class(SO$eqx.effect)
ggplot(data=complete) +
  geom_rect(aes(xmin=date,xmax=date+1,ymin=min(c),ymax=max(c), 
                fill=sig)) +
  geom_line(aes(x=date,y=c),color="green")

plot.ts(time_series[,c(1,12,13)])
## plotting area with no axes
plot(unrate.df, type = "n")
lim <- par("usr")
## adding one rectangle
for (i in 1:nrow(recessions.trim)) {
  rect(recessions.trim[i, 1], lim[3], 
       recessions.trim[i, 2], lim[4], border = "pink", col = "pink")
}
## adding the data
lines(unrate.df)
box()

g = ggplot(complete) + geom_line(aes(x=date, y=c)) + theme_bw()
g = g + geom_rect(data=complete, aes(xmin=, xmax=Trough, ymin=-Inf, ymax=+Inf), fill='pink', alpha=0.2)
plot(complete$c, col=ifelse(complete$sig==1,"blue","red"))

###Impulse Response function
ir.1 <- irf(var.aic,impulse="News.c",response="c",n.ahead = 6,ortho = FALSE, run=1000)
plot(ir.1)



```

# Robustness checks
```{r}
# Test if newspaper articles actually discuss the ECB 
## 1. Take random sample of newspaper articles
set.seed(1993)
random_newspaper_sample = tidy_m[sample(nrow(tidy_m), 100), c(19,25)]
#write.csv(random_newspaper_sample, file="random_coding_sample.csv")

# Import coded sample
coded = read.csv(file="random_coding_sample_coded.csv", header=T)
paste("Newspaper selection precision =", sum(coded[,1]), "% of articles relate to the ECB.")
```

#Figures
```{r}
#figure 1
paste("See section 2.1 and 2.2.")

#figure 2
load(file = "smooth_timeseries")
plot(smooth, main=" ", xlab= " ", cex.lab =0.8)
title(main ="Frame development", sub="y-axis displays fraction of frame words amongst overall wordscount")

#figure 3
load(file = "v.c")
load(file = "v.e")
load(file = "v.u")
#impulse response function
#Create irf, then set shocksize(one forecast error variance) of impulse variable to 100 to get percentage change of response variable
set.seed(1234)

#ECB.core_responsilities
irf_vc = irf(v.c, impulse = "ECB.core_responsibilities", response = "News.core_responsibilities", ortho=T)
irf_vc$irf$ECB.core_responsibilities <- (irf_vc$irf$ECB.core_responsibilities/summary(v.c)$varresult$News.core_responsibilities$sigma)*100
irf_vc$Lower$ECB.core_responsibilities <-(irf_vc$Lower$ECB.core_responsibilities/summary(v.c)$varresult$News.core_responsibilities$sigma)*100
irf_vc$Upper$ECB.core_responsibilities <-(irf_vc$Upper$ECB.core_responsibilities/summary(v.c)$varresult$News.core_responsibilities$sigma)*100
plot(irf_vc)

#News.core_responsilities
irf_vc = irf(v.c, impulse = "News.core_responsibilities", response = "ECB.core_responsibilities", ortho=T)
irf_vc$irf$News.core_responsibilities <- (irf_vc$irf$News.core_responsibilities/summary(v.c)$varresult$ECB.core_responsibilities$sigma)*100
irf_vc$Lower$News.core_responsibilities <-(irf_vc$Lower$News.core_responsibilities/summary(v.c)$varresult$ECB.core_responsibilities$sigma)*100
irf_vc$Upper$News.core_responsibilities <-(irf_vc$Upper$News.core_responsibilities/summary(v.c)$varresult$ECB.core_responsibilities$sigma)*100
plot(irf_vc)

#ECB.expansion_responsilities
irf_ve = irf(v.e, impulse = "ECB.expansion_responsibilities", response = "News.expansion_responsibilities", ortho=T)
irf_ve$irf$ECB.expansion_responsibilities <- (irf_ve$irf$ECB.expansion_responsibilities/summary(v.e)$varresult$News.expansion_responsibilities$sigma)*100
irf_ve$Lower$ECB.expansion_responsibilities <-(irf_ve$Lower$ECB.expansion_responsibilities/summary(v.e)$varresult$News.expansion_responsibilities$sigma)*100
irf_ve$Upper$ECB.expansion_responsibilities <-(irf_ve$Upper$ECB.expansion_responsibilities/summary(v.e)$varresult$News.expansion_responsibilities$sigma)*100
plot(irf_ve)

#News.expansion_responsibilities
irf_ve = irf(v.e, impulse = "News.expansion_responsibilities", response = "News.expansion_responsibilities", ortho=T)
irf_ve$irf$News.expansion_responsibilities <- (irf_ve$irf$News.expansion_responsibilities/summary(v.e)$varresult$ECB.expansion_responsibilities$sigma)*100
irf_ve$Lower$News.expansion_responsibilities <-(irf_ve$Lower$News.expansion_responsibilities/summary(v.e)$varresult$ECB.expansion_responsibilities$sigma)*100
irf_ve$Upper$News.expansion_responsibilities <-(irf_ve$Upper$News.expansion_responsibilities/summary(v.e)$varresult$ECB.expansion_responsibilities$sigma)*100
plot(irf_ve)

#ECB.unconventional_measures
irf_vu = irf(v.u, impulse = "ECB.unconventional_measures", response = "News.unconventional_measures", ortho=T)
irf_vu$irf$ECB.unconventional_measures <- (irf_vu$irf$ECB.unconventional_measures/summary(v.u)$varresult$News.unconventional_measures$sigma)*100
irf_vu$Lower$ECB.unconventional_measures <-(irf_vu$Lower$ECB.unconventional_measures/summary(v.u)$varresult$News.unconventional_measures$sigma)*100
irf_vu$Upper$ECB.unconventional_measures <-(irf_vu$Upper$ECB.unconventional_measures/summary(v.u)$varresult$News.unconventional_measures$sigma)*100
plot(irf_vu)

#News.unconventional_responsilities
irf_vu = irf(v.u, impulse = "News.unconventional_measures", response = "News.unconventional_measures", ortho=T)
irf_vu$irf$News.unconventional_measures <- (irf_vu$irf$News.unconventional_measures/summary(v.u)$varresult$ECB.unconventional_measures$sigma)*100
irf_vu$Lower$News.unconventional_measures <-(irf_vu$Lower$News.unconventional_measures/summary(v.u)$varresult$ECB.unconventional_measures$sigma)*100
irf_vu$Upper$News.unconventional_measures <-(irf_vu$Upper$News.unconventional_measures/summary(v.u)$varresult$ECB.unconventional_measures$sigma)*100
plot(irf_vu)
```

